{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcf3355a-0da6-49cd-943d-3e901d39c8a4",
   "metadata": {},
   "source": [
    "## Import All Libraries For Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9656430d-b3fb-494a-b687-f39f6894a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mtcnn in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mtcnn) (4.6.0.66)\n",
      "Requirement already satisfied: keras>=2.0.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mtcnn) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python>=4.1.0->mtcnn) (1.23.1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f16d48d9-ac3e-410f-b669-74474f34a7f5",
   "metadata": {},
   "source": [
    "## Get Folder Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286030d5-1267-42d6-b28d-d18addf7821c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['face_ms', 'fake_and_real_beers_ms', 'fake_and_real_food_ms', 'fake_and_real_lemon_slices_ms', 'fake_and_real_lemons_ms', 'fake_and_real_peppers_ms', 'fake_and_real_strawberries_ms', 'fake_and_real_sushi_ms', 'fake_and_real_tomatoes_ms', 'feathers_ms', 'flowers_ms', 'glass_tiles_ms', 'hairs_ms']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./complete_ms_data/\"\n",
    "groundtruth_images = []\n",
    "image_size = 512\n",
    "patch_size = 64\n",
    "stride = 32\n",
    "\n",
    "# Get a list of subfolders in the dataset path\n",
    "subfolders = sorted([f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))])\n",
    "\n",
    "print(subfolders)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2aadaa8-15ea-4433-8c55-ccdab80de268",
   "metadata": {},
   "source": [
    "## Import All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c08df5e-599e-4f5a-8c2a-bf10a3d8e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./complete_ms_data/face_ms\n",
      "./complete_ms_data/fake_and_real_beers_ms\n",
      "./complete_ms_data/fake_and_real_food_ms\n",
      "./complete_ms_data/fake_and_real_lemon_slices_ms\n",
      "./complete_ms_data/fake_and_real_lemons_ms\n",
      "./complete_ms_data/fake_and_real_peppers_ms\n",
      "./complete_ms_data/fake_and_real_strawberries_ms\n",
      "./complete_ms_data/fake_and_real_sushi_ms\n",
      "./complete_ms_data/fake_and_real_tomatoes_ms\n",
      "./complete_ms_data/feathers_ms\n",
      "./complete_ms_data/flowers_ms\n",
      "./complete_ms_data/glass_tiles_ms\n",
      "./complete_ms_data/hairs_ms\n",
      "Ground Truth Hyperspectral Images Shape: (2925, 64, 64, 31)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each subfolder\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(dataset_path, subfolder)\n",
    "    print(subfolder_path)\n",
    "\n",
    "    # Stack all the individual images to form a hyperspectral image\n",
    "    hyperspectral_image = np.empty((image_size, image_size, 31), dtype=np.uint8)\n",
    "    i = 0\n",
    "    for filename in sorted(os.listdir(subfolder_path)):\n",
    "        if filename.endswith(\".png\"):\n",
    "            image_file = filename\n",
    "            image_path = os.path.join(subfolder_path, image_file)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            hyperspectral_image[:, :, i] = image\n",
    "            i = i + 1\n",
    "            \n",
    "\n",
    "    # Crop the hyperspectral image into smaller patches\n",
    "    for y in range(0, image_size - patch_size + 1, stride):\n",
    "        for x in range(0, image_size - patch_size + 1, stride):\n",
    "            patch = hyperspectral_image[y:y+patch_size, x:x+patch_size, :]\n",
    "\n",
    "\n",
    "            # Append the cropped patch to the groundtruth_images list\n",
    "            groundtruth_images.append(patch)\n",
    "\n",
    "# Convert the list of ground truth patches to a numpy array\n",
    "groundtruth_arrays = np.array(groundtruth_images)\n",
    "\n",
    "# Print the shape of the ground truth hyperspectral images array\n",
    "print(\"Ground Truth Hyperspectral Images Shape:\", groundtruth_arrays.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e96316a3-2e75-4454-a4a0-e46c6d73a053",
   "metadata": {},
   "source": [
    "## Obtain A Low Resolution HSI Image of Shape (8,8,31)\n",
    "One will simulate an image with low spatial dimensions and high spectral dimension. You'll obtain this by applying a 8,8 averaging filter on each band of your image of resolution 64,64,31 to obtain an image of resolution of 8,8,31. Let's call this one a low(spatial) resolution HSIimage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785c88a-e329-4f92-b210-37755682a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "# Define the size of the averaging filter\n",
    "filter_size = (8, 8)\n",
    "\n",
    "# Apply the averaging filter on each band of the ground truth hyperspectral images\n",
    "lowres_hsi_images = np.zeros((groundtruth_arrays.shape[0], 8, 8, 31))\n",
    "for i in range(groundtruth_arrays.shape[0]):\n",
    "    for j in range(groundtruth_arrays.shape[3]):\n",
    "        band = groundtruth_arrays[i, :, :, j]\n",
    "        lowres_band = uniform_filter(band, size=filter_size)\n",
    "        lowres_hsi_images[i, :, :, j] = lowres_band[::8, ::8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b436545-18ed-425a-906f-340af0be1832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x244030c1300>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(lowres_hsi_images[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cb32d-31b4-483b-905e-ba6073bcf6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-Resolution HSI Images Shape: (2925, 8, 8, 31)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the low-resolution HSI images array\n",
    "print(\"Low-Resolution HSI Images Shape:\", lowres_hsi_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c56fe5-cc77-40f9-8615-f01a0d740693",
   "metadata": {},
   "source": [
    "## Obtain A High Resolution RGB Image of Shape (64,64,3)\n",
    "You'll basically have three 1x1 filters on this image (one will average the bands 1-10, second will average bands 11-20, and the third will average bands 21-31 in the spectral dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6262054c-4b6d-49c2-8af0-7d9d9136b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the 1x1 filters to generate the high-resolution RGB images\n",
    "highres_rgb_images = np.zeros((groundtruth_arrays.shape[0], 64, 64, 3))\n",
    "for i in range(groundtruth_arrays.shape[0]):\n",
    "    for j in range(3):\n",
    "        start_band = j * 10\n",
    "        end_band = (j + 1) * 10\n",
    "        rgb_band_avg = np.mean(groundtruth_arrays[i, :, :, start_band:end_band], axis=-1)\n",
    "        highres_rgb_images[i, :, :, j] = rgb_band_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fd61ffa-0b61-4fbc-8085-2eb0e0122606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24403221720>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(highres_rgb_images[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d89cc63-f337-469c-b451-323e90c1307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Resolution RGB Images Shape: (2925, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the high-resolution RGB images array\n",
    "print(\"High-Resolution RGB Images Shape:\", highres_rgb_images.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64f62ed5-88da-4026-b25b-1128625cf624",
   "metadata": {},
   "source": [
    "## Split Images To Form A Test And Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4f428ef-3096-4a03-9902-83d5346d7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_ratio = 0.8\n",
    "num_samples = groundtruth_arrays.shape[0]\n",
    "num_train_samples = int(train_ratio * num_samples)\n",
    "train_lowres_hsi = lowres_hsi_images[:num_train_samples]\n",
    "train_highres_rgb = highres_rgb_images[:num_train_samples]\n",
    "test_lowres_hsi = lowres_hsi_images[num_train_samples:]\n",
    "test_highres_rgb = highres_rgb_images[num_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183591bd-0f59-4be7-b737-989adfb06e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data (optional but recommended)\n",
    "train_lowres_hsi = train_lowres_hsi / 255.0\n",
    "train_highres_rgb = train_highres_rgb / 255.0\n",
    "test_lowres_hsi = test_lowres_hsi / 255.0\n",
    "test_highres_rgb = test_highres_rgb / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01bb266-a531-4a7c-8811-ffd831d5c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 8, 8, 31)]   0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 64, 64, 31)  61535       ['input_4[0][0]']                \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 34)   0           ['input_3[0][0]',                \n",
      "                                                                  'conv2d_transpose[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 64, 64)   19648       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 64)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 64, 128)  73856       ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64, 64, 128)  0           ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 64, 64, 256)  295168      ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 64, 64, 256)  0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 64, 64, 128)  295040     ['leaky_re_lu_2[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 64, 64, 128)  0           ['conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 64, 64, 64)  73792       ['leaky_re_lu_3[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 64, 64, 64)   0           ['conv2d_transpose_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 64, 64, 3)    1731        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 820,770\n",
      "Trainable params: 820,770\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Concatenate, Conv2D, Conv2DTranspose, BatchNormalization, Activation, LeakyReLU\n",
    "from keras.models import Model\n",
    "\n",
    "def create_super_resolution_model(input_shape_hr, input_shape_lr, output_shape):\n",
    "    # Define high-resolution input layer\n",
    "    hr_input = Input(shape=input_shape_hr)\n",
    "\n",
    "    # Define low-resolution input layer\n",
    "    lr_input = Input(shape=input_shape_lr)\n",
    "\n",
    "    # Upsample the low-resolution input\n",
    "    upsampled_lr = Conv2DTranspose(31, (8, 8), strides=(8, 8), padding='same')(lr_input)\n",
    "\n",
    "    # Concatenate high-resolution and upsampled low-resolution inputs\n",
    "    concat = Concatenate()([hr_input, upsampled_lr])\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), strides=1, padding='same')(concat)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(256, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Conv2DTranspose(128, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2DTranspose(64, (3, 3), strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    output = Conv2D(3, (3, 3), strides=1, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[hr_input, lr_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Define input and output shapes\n",
    "input_shape_hr = (64, 64, 3)\n",
    "input_shape_lr = (8, 8, 31)\n",
    "output_shape = (64, 64, 3)\n",
    "\n",
    "# Create model\n",
    "model = create_super_resolution_model(input_shape_hr, input_shape_lr, output_shape)\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e846fe37-f4bb-4ac4-8754-c899ec56b1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "74/74 [==============================] - 499s 7s/step - loss: 0.0345 - val_loss: 0.0256\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - 343s 5s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - 320s 4s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - 308s 4s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - 311s 4s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - 315s 4s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - 313s 4s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - 300s 4s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - 302s 4s/step - loss: 0.0268 - val_loss: 0.0256\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - 289s 4s/step - loss: 0.0268 - val_loss: 0.0256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243ff062aa0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse')  # Compile the model with appropriate optimizer and loss function\n",
    "\n",
    "model.fit([train_highres_rgb, train_lowres_hsi], train_highres_rgb, validation_data=([test_highres_rgb, test_lowres_hsi], test_highres_rgb), epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
